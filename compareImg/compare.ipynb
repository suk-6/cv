{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009fa4bc-c966-4fae-9fc2-22f416831d2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 탐색 알고리즘 (실패)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e17b4f-00a7-4d04-ab22-c11b5ba96000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c5bd0-1783-4930-acc3-f6cdd203a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(img1, img2):\n",
    "    h, w = img1.shape\n",
    "    diff = cv2.subtract(img1, img2)\n",
    "    err = np.sum(diff**2)\n",
    "    mse = err / (float(h * w))\n",
    "    return mse, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d99f2-fe6a-42b1-80bd-ba699d54ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input videos\n",
    "test = cv2.VideoCapture('test1.mp4')\n",
    "origin = cv2.VideoCapture('test2.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139edac-d34c-4b64-b513-25a7c67a4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video properties (assuming both videos have the same properties)\n",
    "fps = int(test.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(test.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(test.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38299c-1e89-46ea-bf72-31ca8e156672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the frame into 9 regions\n",
    "regions = [\n",
    "    ((0, frame_width // 3), (0, frame_height // 3)),\n",
    "    ((frame_width // 3, 2 * frame_width // 3), (0, frame_height // 3)),\n",
    "    ((2 * frame_width // 3, frame_width), (0, frame_height // 3)),\n",
    "    ((0, frame_width // 3), (frame_height // 3, 2 * frame_height // 3)),\n",
    "    ((frame_width // 3, 2 * frame_width // 3), (frame_height // 3, 2 * frame_height // 3)),\n",
    "    ((2 * frame_width // 3, frame_width), (frame_height // 3, 2 * frame_height // 3)),\n",
    "    ((0, frame_width // 3), (2 * frame_height // 3, frame_height)),\n",
    "    ((frame_width // 3, 2 * frame_width // 3), (2 * frame_height // 3, frame_height)),\n",
    "    ((2 * frame_width // 3, frame_width), (2 * frame_height // 3, frame_height))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e5cfa-cd2f-4128-9ba1-21742580cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frameCounter = 0  # Initialize frame counter\n",
    "\n",
    "# Define the number of regions for comparison\n",
    "num_regions = 9\n",
    "\n",
    "# Define the size of each region (percentage of frame size)\n",
    "region_size = 0.2\n",
    "\n",
    "# Get the size of the frame\n",
    "frame_width = int(test.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(test.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Resize region size based on the frame size\n",
    "region_height = int(frame_height * region_size)\n",
    "region_width = int(frame_width * region_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0088db-e9d1-40f7-b2d6-b108918ada77",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Read frames from both videos\n",
    "    ret1, frame1 = test.read()\n",
    "    ret2, frame2 = origin.read()\n",
    "\n",
    "    # If any of the videos has reached the end, break the loop\n",
    "    if not ret1 or not ret2:\n",
    "        break\n",
    "    \n",
    "    frameCounter += 1  # Increment frame counter\n",
    "\n",
    "    # Compare frames only every 30 frames\n",
    "    if frameCounter % 30 == 0:\n",
    "        # Convert frames to grayscale\n",
    "        grayFrame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        grayFrame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Get the center region of video1 for comparison\n",
    "        center_x1 = grayFrame1.shape[1] // 3\n",
    "        center_x2 = center_x1 * 2\n",
    "        center_y1 = grayFrame1.shape[0] // 3\n",
    "        center_y2 = center_y1 * 2\n",
    "        center_region_frame1 = grayFrame1[center_y1:center_y2, center_x1:center_x2]\n",
    "        \n",
    "        # Compute the size of each region in video2\n",
    "        region_height = int(center_region_frame1.shape[0] * region_size)\n",
    "        region_width = int(center_region_frame1.shape[1] * region_size)\n",
    "\n",
    "        # Extract multiple regions from video2 for comparison\n",
    "        regions_frame2 = []\n",
    "        for i in range(num_regions):\n",
    "            x1 = int((i + 0.5) * (grayFrame2.shape[1] - region_width) / num_regions)\n",
    "            x2 = x1 + region_width\n",
    "            y1 = int((i + 0.5) * (grayFrame2.shape[0] - region_height) / num_regions)\n",
    "            y2 = y1 + region_height\n",
    "            region_frame2 = grayFrame2[y1:y2, x1:x2]\n",
    "            regions_frame2.append(region_frame2)\n",
    "\n",
    "        # Compute MSE for each region in video1 and regions in video2\n",
    "        mse_values = []\n",
    "        for region_frame2 in regions_frame2:\n",
    "            center_region_frame1 = cv2.resize(center_region_frame1, (region_frame2.shape[1], region_frame2.shape[0]))\n",
    "            error, diff = mse(center_region_frame1, region_frame2)\n",
    "            mse_values.append(error)\n",
    "\n",
    "        # Check if the center region of video1 belongs to video2\n",
    "        threshold = 75  # Adjust this threshold based on your data\n",
    "        is_center_included = any(mse_value > threshold for mse_value in mse_values)\n",
    "\n",
    "        # Visualization (You can modify this part as needed)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Test Video\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.imshow(cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Origin Video - Center Included: {is_center_included}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.imshow(cv2.cvtColor(center_region_frame1, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Test Video - Cropped\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.imshow(cv2.cvtColor(regions_frame2[randint(0, num_regions - 1)], cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Origin Video - Cropped\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        clear_output(wㅠait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad50209-b3a8-46a0-b7d4-70fd15cea9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release video capture objects\n",
    "test.release()\n",
    "origin.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15d0d0-9487-4c02-bb3f-2bb3876ba355",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SIFT (성공)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958eb566-cdb1-4d59-811a-b8c54ce5dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e1042-0e3a-4198-9216-6bd0131f213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_points(video1Path, video2Path):\n",
    "    # Load the input videos\n",
    "    video1 = cv2.VideoCapture(video1Path)\n",
    "    video2 = cv2.VideoCapture(video2Path)\n",
    "\n",
    "    # SIFT 특징점 검출\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    frameCounter = 0\n",
    "\n",
    "    while True:\n",
    "        # Read frames from both videos\n",
    "        ret1, frame1 = video1.read()\n",
    "        ret2, frame2 = video2.read()\n",
    "\n",
    "        # If any of the videos has reached the end, break the loop\n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "            \n",
    "        frameCounter += 1  # Increment frame counter\n",
    "\n",
    "        # Compare frames only every 30 frames\n",
    "        if frameCounter % 180 == 0:\n",
    "\n",
    "            # Convert frames to grayscale\n",
    "            grayFrame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "            grayFrame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # SIFT 특징점 검출 및 매칭\n",
    "            kp1, des1 = sift.detectAndCompute(grayFrame1, None)\n",
    "            kp2, des2 = sift.detectAndCompute(grayFrame2, None)\n",
    "\n",
    "            # 특징점 매칭\n",
    "            bf = cv2.BFMatcher()\n",
    "            matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "            # 거리가 가까운 매칭 결과 선택\n",
    "            goodMatches = []\n",
    "            for m, n in matches:\n",
    "                if m.distance < 0.75 * n.distance:\n",
    "                    goodMatches.append(m)\n",
    "\n",
    "            # 유사점 그리기\n",
    "            resultImg = cv2.drawMatches(grayFrame1, kp1, grayFrame2, kp2, goodMatches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "            \n",
    "            title = f\"Number of similarities: {len(goodMatches)}\" if len(goodMatches) > 300 else f\"Number of similarities: {len(goodMatches)}\\nGet off the road\"\n",
    "\n",
    "            # Show the result\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            plt.imshow(resultImg)\n",
    "            plt.title(title)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    # Release video capture objects\n",
    "    video1.release()\n",
    "    video2.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9ac35-9c43-495a-ba69-f21b52765bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    video1_path = \"test1.mp4\"\n",
    "    video2_path = \"test2.mp4\"\n",
    "    find_similar_points(video1_path, video2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0941e4b6-08c8-4825-a244-f5d0233bb83b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
